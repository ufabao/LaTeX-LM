{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\micah\\Desktop\\mathtest.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 256\n",
    "n_embd = 512\n",
    "head_size = 64\n",
    "n_heads = 8\n",
    "depth = 8\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 1e-2\n",
    "betas = (0.9, 0.99)\n",
    "eps = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for i, s in enumerate(vocab)}\n",
    "tokenize = lambda s: torch.tensor([stoi[c] for c in s])\n",
    "detokenize = lambda c: ''.join([itos[x] for x in c])\n",
    "\n",
    "data = tokenize(text)\n",
    "n = int(len(data)*.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+1 + seq_len] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "    def __init__(self, text, seq_len=seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.data = text\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx: idx + seq_len], self.data[idx+1: idx + seq_len + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Linear(n_embd, n_embd)\n",
    "    def forward(self, x):\n",
    "        y = self.weight(x)\n",
    "        return F.silu(x)*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn \n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        y = self.fn(x, **kwargs)\n",
    "        x = self.drop(x)\n",
    "        return y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, n_embd=n_embd):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, head_dim: int, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = float(head_dim)/(base ** torch.arange(0, head_dim, 2).float())\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.head_dim = head_dim\n",
    "        self.seq_len_cached = None\n",
    "        self.batch_size_cached = None\n",
    "        self.cos_cached: torch.tensor | None = None\n",
    "        self.sin_cached: torch.tensor | None = None\n",
    "\n",
    "    def trig(self, seq_len: int, device=device, dtype=torch.bfloat16) -> torch.Tensor:\n",
    "        if seq_len != self.seq_len_cached: \n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum('i,j -> ij', t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).float().to(device)\n",
    "\n",
    "            self.cos_cached = emb.cos()\n",
    "            self.sin_cached = emb.sin()\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        _, _, seq_len, _ = q.shape\n",
    "        cos, sin = self.trig(seq_len)\n",
    "        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttentionHead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, n_embd, attention_drop = 0.1, ff_drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_drop = nn.Dropout(attention_drop)\n",
    "        self.qkv = nn.Linear(n_embd, n_heads*head_size + 2*head_size)\n",
    "        self.rotary = RotaryEmbedding(head_size)\n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.ff_out = nn.Sequential(\n",
    "            SwiGLU(n_embd=n_embd),\n",
    "            nn.Dropout(ff_drop),\n",
    "            nn.Linear(n_heads * head_size, n_embd, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (B, T, E) ---> (B, num_heads, T, h_size)\n",
    "        B, T, E = x.shape\n",
    "        x = self.LNorm(x)\n",
    "\n",
    "        qkv = self.qkv(x) #(B, T, n_heads*head_size + 2*head_size)\n",
    "\n",
    "        # q has shape (B, n_heads, T, head_size)\n",
    "        q = qkv[:, :, : n_heads*head_size].view((B, T, n_heads, head_size)).transpose(-2, -3)\n",
    "        # k has shape (B, T, head_size)\n",
    "        k = qkv[:, :, n_heads*head_size:n_heads*head_size+head_size].view((B, 1, T, head_size))\n",
    "        # v has shape (B, T, head_size)\n",
    "        v = qkv[:, :, -head_size:].view((B, 1, T, head_size))\n",
    "        \n",
    "        q, k = self.rotary(q, k)\n",
    "\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.1, is_causal=True)\n",
    "        y = einops.rearrange(y, 'b h t d -> b t (h d)')\n",
    "        return self.ff_out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXModel(nn.Module):\n",
    "    def __init__(self, depth, n_heads, head_size, n_embd=n_embd, vocab_size=vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding = Token_Embedding(n_embd = n_embd)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            block = Residual(MultiQueryAttentionHead(n_heads, head_size, n_embd))\n",
    "            self.layers.append(block)\n",
    "        \n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.to_logits = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        #self.to_logits.weight = self.token_embedding.weight\n",
    "        #nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -seq_len:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.token_embedding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        embeds = self.LNorm(x)\n",
    "\n",
    "        logits = self.to_logits(embeds)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, seq_len = seq_len):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.epochs = 5\n",
    "        self.dataset = data(train_data, seq_len)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.model = LaTeXModel(depth, n_heads, head_size)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = lr, weight_decay = wd, betas = betas, eps = eps)\n",
    "        \n",
    "    def train(self):\n",
    "        for iter in range(self.epochs):\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                data, target = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                self.model.train()\n",
    "                _, loss = self.model(data, target)\n",
    "                #if i % 100 == 0:\n",
    "                    #print('{i/}'loss)\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = Trainer()\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaTeXModel(depth, n_heads, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = wd, betas = betas, eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss():\n",
    "    eval_iters = 10\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 1000\n",
    "for iter in range(20000):\n",
    "    #print training progress\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    #update\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\teCtT_yY_}_p_Oi{,Y_ _p\n",
      "\\$(a a (wiR)}_eY^eX_r)sf.\\,$t\n",
      "}c &f(_X)( Yp^ M)r\\(K)+Ua {_hXf\\Jac-A(c,Ci (M)_(MS)=,NKb)2(-X)$E\\\\\\t, P) - a)  \\'E)\n",
      "\\t b_{\n",
      "+Zu}, k(M(c) = \\beta(f_1) + y^*y \\ect(\n",
      "\\begi^*M) \\gamma_1^* \\to  \\textit{Ab} a + \\epsilon +\n",
      "\\mathbf{Q}(d(J^{-} \\beta)) + j \\geq\n",
      "\\bar\\{\\quad\\text{and}\\quad\n",
      "a + \\text{and}\\quad\n",
      "g + \\epsilon + m (\\text{ such that }(d(T))), \\alpha, s, t, t, w)\n",
      "$$\n",
      "because the except for every Fund let $\\xi A$ are except we additive to that $U$\n",
      "is compatible,\n",
      "we having afinitely many ore maineral. Suppose a vector space of\n",
      "$\\mathcal{S}$ by View\" as an object of $\\Spec(A)$. Let $x'$ be topological\n",
      "corresponding to $Y' \\to Y$ such that $U$ is scheme.\n",
      "Choose a square of $\\mathcal{U} : X \\to X' \\to X$.\n",
      "An inductions $\\mathcal{V}'_A$ and the following collowing\n",
      "properties:\n",
      "\\begin{enumerate}\n",
      "\\item $X'$ has a property (1) the corresponding category $\\mathcal{V}$ are\n",
      "constructible induced affined, see Cohow'hlover-Mier-D}.\n",
      "\\item Given a the a fibre product\n",
      "$$\n",
      "\\Omega^d_{X'/S}' \\subset S \\longrightarrow \\Omega^d_{Y_1/S}/S.\n",
      "$$\n",
      "\\end{enumerate}\n",
      "\\end{proposition}\n",
      "\n",
      "\\begin{proof}\n",
      "Clar morphism $S \\to S \\to S$ is surjective of a long exact sequence, s\n",
      "\\begin{equation}\n",
      "\\label{ifying-sections-left}\n",
      "\\begin{slogan}\n",
      "Suppartially interprete, let\n",
      "$\\mathcal{U}$ be a perfect fully faithfulbered by a pair\n",
      "of construction. A Artining exact sequence\n",
      "$$\n",
      "0 \\to R^{\\oplustnic} \\to K \\otimes^{\\spartial}_A \\to K \\to \\ldots\n",
      "\\otimes_A K \\to \\ldots\n",
      "\\otimes^{\\oplus r} \\to 0\n",
      "$$\n",
      "of connected lemma with\n",
      "Algebra, Lemma \\ref{algebra-lemma-analyters-arkover-exactness} we have\n",
      "\\begin{enumerate}\n",
      "\\item the functor is exact.\n",
      "\\item since $A = K'/(L' \\times_K L)$ is an isomorphism\n",
      "(to be an invertible sheafification on $K$.\n",
      "\\end{enumerate}\n",
      "The discussion follows from\n",
      "Lemmas \\ref{lemma-affine-diagram-projective-diagonal-to-scheme},\n",
      "\\ref{lemma-weakly-injective-dimension-formuls},\n",
      "\\ref{lemma-section-affine},\n",
      "\\ref{lemma-resoriction-full-scheme-topoi},\n",
      "\\ref{lemma-weil-stalk-propert-dp},\n",
      "and \\ref{lemma-can-polynomial-pelation}\n",
      "the resurt has section exists and only morphisms (\\ref{lemma-restrict-de-1})\n",
      "and as well is equal to a lift.\n",
      "Hence\n",
      "(\\ref{equation-map})\n",
      "with the pense of the play of the condity assertion.\n",
      "Namely, hint: for any stack avcent of opens $V$ we have already\n",
      "definition.\n",
      "\n",
      "\\medskip\\noindent\n",
      "By made, the choices we prove we set $V = X$, resp.\n",
      "Considel over field where we have the field two nonemption\n",
      "hypothdes of itherwise conditions opens.\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{lemma-characterize-pre-lined-order-thickeninb}\n",
      "Let $f : X \\to Y$ be a morphism of schemes. Let\n",
      "$A$ be the integral closurd on $Y$ such that $X_{Zar}$ is exact\n",
      "(Lemma \\ref{lemma-integral-normanence-morphism}). Let $A \\to A^n \\to B^n \\to B^n \\to B^n$\n",
      "be a homomorphism of schemes. Let $f_a \\in b \\cap D^m$.\n",
      "\\begin{equation}\n",
      "Set $B^{nt} \\to D_C$ be a flat presentation.\n",
      "Then\n",
      "\\begin{enumerate}\n",
      "\\item (1) has an $A$-module wuitably be the descrabed category of\n",
      "the $A$-module of families $J' \\subset C$. However, if\n",
      "an element $C$ we may working $I' = M'$, $N' = N''$, $B''$ and $N'$, and $A$ is Curjective.\n",
      "\\item The pushout $J/\\mathfrak r$ of\n",
      "Sterdinitial of the multiplicative factors\n",
      "$$\n",
      "\\bigoplus\\nolimits_{\\alpha = 0}\n",
      "\\mathfrak r = \\colim  \\text{pr}_R\n",
      "K^\\wedge_{\\mathfrak r \\leq 0} R\n",
      "\\xrightarrow{\\text{pr}_{1 \\Leq 2}K^\\wedge^\\wedge K^\\wedge}\n",
      "\\longrightarrow\n",
      "M\n",
      "$$\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{proof}\n",
      "Jaccally, we may and check $\\mathfrak r/\\mathfrak p \\subset \\mathfrak r$ and\n",
      "Algebra, Lemma \\ref{algebra-lemma-ideal-prime}\n",
      "maximal ideal ideal $\\mathfrak q$ and which\n",
      "map to zero in $\\mathfrak q A$. Plearly induces an isomorphism on (rother any).\n",
      "As above there is a local formal ideal and completion $R' \\to R''$ with\n",
      "the $\\mathfrak q''$-algebra $\\mathfrak q' \\subset R$\n",
      "\\end{example}.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\section{Annonstandivivision functors}\n",
      "\\label{section-diagonal}\n",
      "\\begin{reference}\n",
      "\n",
      "\\noindent\n",
      "In this section we get that this morphism is local. To defed it dependix\n",
      "for $S' = S'/\\mathfrak q'$, if we assume that $S'/R$ satisfies\n",
      "the case of dimension $\\leq d \\mathfrak q$, it is necessary. Hence we\n",
      "may assume (2) and (2). For the rest our saccesstrt. By\n",
      "Stackes of locus in parts (5) we see (2) above that $X'$ is closed.\n",
      "Let $\\omega_S$ be a duality of complexes above we can choose\n",
      "separate of $x \\in |(S''|)$ and be guaranted by $|X|$. Cle part (1) and let $|/X''$\n",
      "be as over $X' \\setminus X'$. The diagram\n",
      "$$\n",
      "X_{\\opi} \\to Y \\to X''_{\\overline{X}} \\to Y'\n",
      "$$\n",
      "is surjective. We can off\n",
      "$$\n",
      "\\xymatrix{\n",
      "X \\ar[d]_g \\ar[r]_{K \\ar[d]_f & X \\ar[d]^a \\\\\n",
      "X' \\ar[r]^{j'} & X' \\ar[l]_a/X' \\ar[d]^\\oplus \\overline{b}'\n",
      "}\n",
      "$$\n",
      "with one obvioven for $g \\in A$ and $a \\in A/fA$, $b \\in A_g B$ whictually generate\n",
      "$K(\\mathfrak p) = \\sum a_mat(x_i) = \\sum_i t_i$.\n",
      "Hence the following are equivalent:\n",
      "\\end{proposition}\n",
      "\n",
      "\\begin{proof}\n",
      "The category of cohomology groups of local rings on $D(\\mathcal{A})$ are isomorphisms\n",
      "and the unber complex of $\\mathcal{B}$.\n",
      "\\end{proposition}\n",
      "\n",
      "\\begin{proposition}\n",
      "\\label{proposition-category-bhowder}\n",
      "Let $S$ be a scheme over $S$.\n",
      "Let $f : \\mathcal{A} \\to \\mathcal{D}$ with $g$ locally finite type\n",
      "dimension for quasi-regular, resp.\n",
      "Let $\\mathcal{F}$ be a finite Galois constructaining int.\n",
      "There exists an exist $c$ and $\\mathcal{F}'$ bight adjsing\n",
      "\\betale existence at $x \\in H^1(X, \\mathcal{F})$ commute. This follows\n",
      "defines an in the proof).\n",
      "\n",
      "\\medskip\\noindent\n",
      "such pair of distinguisible smooth and this is as defined in the next\n",
      "lemma follows. Since $\\mathcal{F} \\to \\mathcal{E}$,\n",
      "where $\\mathcal{F}$ is abbit no\n",
      "$\\mathcal{E}_\\varphi$ at Zariski local set at some $Z$\n",
      "we have the map\n",
      "$$\n",
      "(\\itlan}\\mathcal{O}_Z)^{\\oplus n \\mathcal{E}} \\to \\mathcal{E}_\\pi^{\\oplus p} \\to \\mathcal{E}/\\mathcal{E}\n",
      "$$\n",
      "of finite long, such that $\\psi_\\infty^p$ is the remark of\n",
      "the finishit we short\n",
      "finite modules\n",
      "$$\n",
      "\\text{Trace}_X(\\mathcal{E}/\\lan \\mathcal{E})\n",
      "\\to \\text{Trace}_{fpqr}(\\omega)\n",
      "$$\n",
      "Artin's a simplicial is a ring on $R$. By\n",
      "Lemma \\ref{lemma-genus-at-closed-subscheme}\n",
      "we just displarity of formally products of clarced subschemes of ringed\n",
      "spaces we use the fieldsure of\n",
      "$R/\\mathfrak m^{sh}$ and it is the result when with notation\n",
      "can be coequal to the projective diagram in the proof of\n",
      "Lemma \\ref{lemma-isomorphism-affine}.\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{lemma-homotopy-functoromod-pense-proj-after-immersion}\n",
      "Let $S$ be a scheme. Let $f : X \\to Y$ be a morphism of finite presentation over $S$.\n",
      "Then\n",
      "$f^* : \\text{p}_1^*\\mathcal{F} \\otimes_k Y_i$ is an object\n",
      "of affine on $S' \\to \\mathcal{G}_i$ whose map is and hence\n",
      "to esourt.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{proof}\n",
      "Let $\\mathcal{H} = \\Omega^n_{f : \\wedge^{n - 1}_i$ be the $I$-adic completion\n",
      "by Lemma \\ref{lemma-polynomial-in-neintreasz}.\n",
      "This follows from Lemma \\ref{lemma-normal}.\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{lemma-gysin-Gorenstein}\n",
      "In Siccus\\fequentesp{\\'Etale-gooduct-class}\n",
      "In Situation \\ref{situation-locally-section} let $(V_j, v)$ be a\n",
      "bigraded ring and let $R \\to U_i$ be a morphism of finite number of\n",
      "genus of modules. Then there exists a sequence of objects $q'$ and an open neighbourhood,\n",
      "which is exact algebraic so if and only if\n",
      "and only if the morphism\n",
      "$V_{i_j} \\cap V_{i_j} \\to U_{i_j}'$ of\n",
      "Morphisms, Lemmas \\ref{morphisms-lemma-algebraize-weakly-etale} are as acyclics.\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{remark}\n",
      "\\text{rect}\n",
      "\\label{remark-good-renumber-base-permanence-and-better}\n",
      "Let $r \\in R \\mapsto s_i, y_i, a \\geq 0}$ mapping of\n",
      "invertible for all $b - i$. We see that\n",
      "$\\Mor_{\\mathcal{O}_{\\Spec(R)}}(R, \\Spec(R), U) = s, i'$. Define\n",
      "$U_i \\to S$ and $D_{(\\Sch/S})$ a scheme over $S$ with the (Sembearly on-valuative\n",
      "ring finite tor modules is cartinged the on the Noether\n",
      "does not compatible with equality). Hence we may assume that the cartesion\n",
      "map comes modules of $R$ and $X_i = S \\otimes_R S$ are locally on the maps\n",
      "on $f$. We will be the stratification of the scheme\n",
      "$X_{i, \\etale}$ consisting of any discrete valuation ring\n",
      "mean? The completion $X$ is the sheafification of this is projective by\n",
      "Modules, Lemma \\ref{modules-lemma-descend-directed}.\n",
      "Then $X_T$ is fppf the same base case (Lemma \\ref{lemma-characterize-valuation-normal}).\n",
      "If $X_T \\to \\Spec(A)$ is flat (in full\n",
      "smooth over $s$, then the final of coxible modules on $X$\n",
      "if $\\Omega_{T/A}^m_T/A$ has kernel helds.\n",
      "Hence converse on Algebra, Lemma\n",
      "\\ref{algebra-lemma-ler-mod-versal}.\n",
      "\\end{proof}\n",
      "\n",
      "\\noindent\n",
      "The module $\\Omega_{P/S}$ is generated. First, locally \\'etale sequence\n",
      "in it suffices to free, for the certakence closed subscheme of\n",
      "the image $Z_s$. Since $Z_s$ is disjon by $\\text{Mod}_{\\mathcal{O}_{P/B}$\n",
      "we have $R = \\mathcal{O}_{P_s}$ and $R^{\\otimes r} = R^n\\mathbf{Z}$.\n",
      "In particular we wild as we have the ring\n",
      "$k = \\prod_{t \\in k} 1 \\otimes \\mathcal{O}_{P_s}$\n",
      "is subset of respective as the topological space of the open subset.\n",
      "The less of the root is whos the essential truncation in Spaces, Section\n",
      "\\ref{spaces-means-when-ever}. Thus may abstrassing $[\\mathcal{O}_{P_S}]_k, [\\mathcal{O}_{P_s}]$\n",
      "for zero poly of sections with $k = k$ that it see the\n",
      "map $\\mathcal{F}^{k + 1}\\mathcal{F} \\to \\mathcal{G}^m$,\n",
      "see Theorem \\ref{theorem-trick-to-}. For every maximal idempotent $\\mathcal{F}^m$\n",
      "of finite presentation $\\mathcal{G}[\\exist{T}]_\\mathcal{G}^n$\n",
      "are changed fibre]. The same is true by\n",
      "Modules, Lemma \\ref{modules-lemma-quasi-local}. However, if\n",
      "$\\mathcal{G}$ is analytically ural, and, thence for all existence of $f$\n",
      "on $X_\\etale$ over a basis. Set $(n) \\subset X$ for the success\n",
      "are $X$-schemes well often paragraph, we may conclude\n",
      "that $X$ has finite locally free of dimension $\\leq c$.\n",
      "\\end{enumerate}\n",
      "\\end{proposition}\n",
      "\n",
      "\\begin{proof}\n",
      "Since lemma in degree that after both that any ring maps $S_0 \\to X$ correspond\n",
      "we see that $S_0 \\to S_0$ is isomorphic to\n",
      "the factorization\n",
      "for (small effections of $C: X \\to S$ in $S$).\n",
      "Observe to prove the in this section with taking convention them.\n",
      "\n",
      "\\medskip\\noindent\n",
      "This work on freelens, take i.e., is exact, maps see\n",
      "or that $C \\cap C_0$\n",
      "and $C/Z$ via the claim condition spots of pairswisy in \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 128), dtype=torch.long).to(device)\n",
    "print(detokenize(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6928896"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE\n",
    "#MODEL\n",
    "#SAVE SAVE SAVE\n",
    "#torch.save(model, r'C:\\Users\\micah\\Desktop\\pytorch_models\\falcon256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD\n",
    "#MODEL\n",
    "#LOAD LOAD LOAD\n",
    "#model = torch.load(r'C:\\Users\\micah\\Desktop\\pytorch_models\\falcon256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
