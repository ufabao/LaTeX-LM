{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"\\Users\\micah\\Desktop\\tiny-shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 256\n",
    "n_embd = 384\n",
    "head_size = 64\n",
    "n_heads = 8\n",
    "depth = 6\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 1e-2\n",
    "betas = (0.9, 0.99)\n",
    "eps = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for i, s in enumerate(vocab)}\n",
    "tokenize = lambda s: torch.tensor([stoi[c] for c in s])\n",
    "detokenize = lambda c: ''.join([itos[x] for x in c])\n",
    "\n",
    "data = tokenize(text)\n",
    "n = int(len(data)*.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "    def __init__(self, text, seq_len=seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.data = text\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx: idx + seq_len], self.data[idx+1: idx + seq_len + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beta = nn.parameter(torch.ones())\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn \n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        y = self.fn(x, **kwargs)\n",
    "        x = self.drop(x)\n",
    "        return y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, n_embd=n_embd):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, head_dim: int, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = float(head_dim)/(base ** torch.arange(0, head_dim, 2).float())\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.head_dim = head_dim\n",
    "        self.seq_len_cached = None\n",
    "        self.batch_size_cached = None\n",
    "        self.cos_cached: torch.tensor | None = None\n",
    "        self.sin_cached: torch.tensor | None = None\n",
    "\n",
    "    def trig(self, seq_len: int, device=device, dtype=torch.bfloat16) -> torch.Tensor:\n",
    "        if seq_len != self.seq_len_cached: \n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum('i,j -> ij', t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).float().to(device)\n",
    "\n",
    "            self.cos_cached = emb.cos()\n",
    "            self.sin_cached = emb.sin()\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        _, _, seq_len, _ = q.shape\n",
    "        cos, sin = self.trig(seq_len)\n",
    "        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttentionHead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, n_embd, attention_drop = 0.1, ff_drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_drop = nn.Dropout(attention_drop)\n",
    "        self.qkv = nn.Linear(n_embd, n_heads*head_size + 2*head_size)\n",
    "        self.rotary = RotaryEmbedding(head_size)\n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.ff_out = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(ff_drop),\n",
    "            nn.Linear(n_heads * head_size, n_embd, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (B, T, E) ---> (B, num_heads, T, h_size)\n",
    "        B, T, E = x.shape\n",
    "        x = self.LNorm(x)\n",
    "\n",
    "        qkv = self.qkv(x) #(B, T, n_heads*head_size + 2*head_size)\n",
    "\n",
    "        # q has shape (B, n_heads, T, head_size)\n",
    "        q = qkv[:, :, : n_heads*head_size].view((B, T, n_heads, head_size)).transpose(-2, -3)\n",
    "        # k has shape (B, T, head_size)\n",
    "        k = qkv[:, :, n_heads*head_size:n_heads*head_size+head_size].view((B, 1, T, head_size))\n",
    "        # v has shape (B, T, head_size)\n",
    "        v = qkv[:, :, -head_size:].view((B, 1, T, head_size))\n",
    "        \n",
    "        q, k = self.rotary(q, k)\n",
    "\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.1, is_causal=True)\n",
    "        y = einops.rearrange(y, 'b h t d -> b t (h d)')\n",
    "        return self.ff_out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXModel(nn.Module):\n",
    "    def __init__(self, depth, n_heads, head_size, n_embd=n_embd, vocab_size=vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding = Token_Embedding(n_embd = n_embd)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            block = Residual(MultiQueryAttentionHead(n_heads, head_size, n_embd))\n",
    "            self.layers.append(block)\n",
    "        \n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.to_logits = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        #self.to_logits.weight = self.token_embedding.weight\n",
    "        #nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -seq_len:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.token_embedding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        embeds = self.LNorm(x)\n",
    "\n",
    "        logits = self.to_logits(embeds)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, seq_len = seq_len):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.epochs = 5\n",
    "        self.dataset = data(train_data, seq_len)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.model = LaTeXModel(depth, n_heads, head_size)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = lr, weight_decay = wd, betas = betas, eps = eps)\n",
    "        \n",
    "    def train(self):\n",
    "        for iter in range(self.epochs):\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                data, target = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                self.model.train()\n",
    "                _, loss = self.model(data, target)\n",
    "                if i % 100 == 0:\n",
    "                    print('{i/}'loss)\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7217, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer()\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(loss)\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGCIOvStoMC\n",
      "WncaOCLOPOROtMWerNGoCulaftswaWioOfrosowhLonUKIOHiuavo,OviGLABUKaromilprdo'Gres hed HERARNolonovascarond:\n",
      "Folounincond?\n",
      "Tond SCUCetel myopto'st uourersh!\n",
      "DUSTUTIO:\n",
      "Ser,\n",
      "A LoGod, thary 'Wham in tlaut onot anee eartle therery what buth ista's ur heart,\n",
      "The father's to you lovercy? dive disposs pon\n",
      "Look ison. O's hid in thou chatly canised\n",
      "A not, in so for gron; he nothan come\n",
      "Your ther side mame minegualdner hou woul at\n",
      "There blown by thee falk spoke of\n",
      "Affect the joy ly counter trive o\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 128), dtype=torch.long).to(device)\n",
    "print(detokenize(trainer.model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DataLoader(c, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[47, 58, 46,  1, 58, 46, 47, 52, 43, 11,  0, 13, 52, 42,  1, 47, 52,  1,\n",
      "         58, 46, 47, 57,  1, 60, 53, 61,  1, 42, 53,  1, 41, 46, 39, 47, 52,  1,\n",
      "         51, 63,  1, 57, 53, 59, 50,  1, 58, 53,  1, 58, 46, 47, 52, 43,  2,  0,\n",
      "         13, 52, 42,  6,  1, 43, 56, 43,  1, 51, 63,  1, 49, 52, 43, 43,  1, 56,\n",
      "         47, 57, 43,  1, 44, 56, 53, 51,  1, 58, 46, 43,  1, 43, 39, 56, 58, 46,\n",
      "          5, 57,  1, 41, 53, 50, 42,  1, 44, 39, 41, 43,  6,  0, 21,  1, 58, 46,\n",
      "         56, 53, 61,  1, 51, 63,  1, 46, 39, 52, 42, 57,  6,  1, 51, 47, 52, 43,\n",
      "          1, 43, 63, 43, 57,  6,  1, 51, 63,  1, 46, 43, 39, 56, 58,  1, 58, 53,\n",
      "          1, 58, 46, 43, 43,  6,  0, 32, 46, 53, 59,  1, 57, 43, 58, 58, 43, 56,\n",
      "          1, 59, 54,  1, 39, 52, 42,  1, 54, 50, 59, 41, 49, 43, 56,  1, 42, 53,\n",
      "         61, 52,  1, 53, 44,  1, 49, 47, 52, 45, 57,  6,  0, 14, 43, 57, 43, 43,\n",
      "         41, 46, 47, 52, 45,  1, 58, 46, 43, 43,  6,  1, 47, 44,  1, 61, 47, 58,\n",
      "         46,  1, 58, 46, 43, 63,  1, 61, 47, 50, 50,  1, 47, 58,  1, 57, 58, 39,\n",
      "         52, 42, 57,  0, 32, 46, 39, 58,  1, 58, 53,  1, 51, 63,  1, 44, 53, 43,\n",
      "         57,  1, 58, 46]])\n",
      "\n",
      "\n",
      "tensor([[58, 46,  1, 58, 46, 47, 52, 43, 11,  0, 13, 52, 42,  1, 47, 52,  1, 58,\n",
      "         46, 47, 57,  1, 60, 53, 61,  1, 42, 53,  1, 41, 46, 39, 47, 52,  1, 51,\n",
      "         63,  1, 57, 53, 59, 50,  1, 58, 53,  1, 58, 46, 47, 52, 43,  2,  0, 13,\n",
      "         52, 42,  6,  1, 43, 56, 43,  1, 51, 63,  1, 49, 52, 43, 43,  1, 56, 47,\n",
      "         57, 43,  1, 44, 56, 53, 51,  1, 58, 46, 43,  1, 43, 39, 56, 58, 46,  5,\n",
      "         57,  1, 41, 53, 50, 42,  1, 44, 39, 41, 43,  6,  0, 21,  1, 58, 46, 56,\n",
      "         53, 61,  1, 51, 63,  1, 46, 39, 52, 42, 57,  6,  1, 51, 47, 52, 43,  1,\n",
      "         43, 63, 43, 57,  6,  1, 51, 63,  1, 46, 43, 39, 56, 58,  1, 58, 53,  1,\n",
      "         58, 46, 43, 43,  6,  0, 32, 46, 53, 59,  1, 57, 43, 58, 58, 43, 56,  1,\n",
      "         59, 54,  1, 39, 52, 42,  1, 54, 50, 59, 41, 49, 43, 56,  1, 42, 53, 61,\n",
      "         52,  1, 53, 44,  1, 49, 47, 52, 45, 57,  6,  0, 14, 43, 57, 43, 43, 41,\n",
      "         46, 47, 52, 45,  1, 58, 46, 43, 43,  6,  1, 47, 44,  1, 61, 47, 58, 46,\n",
      "          1, 58, 46, 43, 63,  1, 61, 47, 50, 50,  1, 47, 58,  1, 57, 58, 39, 52,\n",
      "         42, 57,  0, 32, 46, 39, 58,  1, 58, 53,  1, 51, 63,  1, 44, 53, 43, 57,\n",
      "          1, 58, 46, 47]])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(x):\n",
    "    print(batch[0])\n",
    "    print('\\n')\n",
    "    print(batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OXFORD:\\nAway, away, to meet the queen's great power!\\n3 KING HENRY VI\\n\\nKING EDWARD IV:\\nThus far our fortune keeps an upward course,\\nAnd we are graced with wreaths of victory.\\nBut, in the midst of this bright-shining day,\\nI spy a black, suspicious, threateni\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize([27, 36, 18, 27, 30, 16, 10,  0, 13, 61, 39, 63,  6,  1, 39, 61, 39, 63,\n",
    "         6,  1, 58, 53,  1, 51, 43, 43, 58,  1, 58, 46, 43,  1, 55, 59, 43, 43,\n",
    "        52,  5, 57,  1, 45, 56, 43, 39, 58,  1, 54, 53, 61, 43, 56,  2,  0,  9,\n",
    "         1, 23, 21, 26, 19,  1, 20, 17, 26, 30, 37,  1, 34, 21,  0,  0, 23, 21,\n",
    "        26, 19,  1, 17, 16, 35, 13, 30, 16,  1, 21, 34, 10,  0, 32, 46, 59, 57,\n",
    "         1, 44, 39, 56,  1, 53, 59, 56,  1, 44, 53, 56, 58, 59, 52, 43,  1, 49,\n",
    "        43, 43, 54, 57,  1, 39, 52,  1, 59, 54, 61, 39, 56, 42,  1, 41, 53, 59,\n",
    "        56, 57, 43,  6,  0, 13, 52, 42,  1, 61, 43,  1, 39, 56, 43,  1, 45, 56,\n",
    "        39, 41, 43, 42,  1, 61, 47, 58, 46,  1, 61, 56, 43, 39, 58, 46, 57,  1,\n",
    "        53, 44,  1, 60, 47, 41, 58, 53, 56, 63,  8,  0, 14, 59, 58,  6,  1, 47,\n",
    "        52,  1, 58, 46, 43,  1, 51, 47, 42, 57, 58,  1, 53, 44,  1, 58, 46, 47,\n",
    "        57,  1, 40, 56, 47, 45, 46, 58,  7, 57, 46, 47, 52, 47, 52, 45,  1, 42,\n",
    "        39, 63,  6,  0, 21,  1, 57, 54, 63,  1, 39,  1, 40, 50, 39, 41, 49,  6,\n",
    "         1, 57, 59, 57, 54, 47, 41, 47, 53, 59, 57,  6,  1, 58, 46, 56, 43, 39,\n",
    "        58, 43, 52, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
