{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\micah\\Desktop\\mathtest.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 256\n",
    "n_embd = 512\n",
    "head_size = 64\n",
    "n_heads = 8\n",
    "depth = 8\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 1e-2\n",
    "betas = (0.9, 0.99)\n",
    "eps = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for i, s in enumerate(vocab)}\n",
    "tokenize = lambda s: torch.tensor([stoi[c] for c in s])\n",
    "detokenize = lambda c: ''.join([itos[x] for x in c])\n",
    "\n",
    "data = tokenize(text)\n",
    "n = int(len(data)*.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+1 + seq_len] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "    def __init__(self, text, seq_len=seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.data = text\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx: idx + seq_len], self.data[idx+1: idx + seq_len + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Linear(n_embd, n_embd)\n",
    "    def forward(self, x):\n",
    "        y = self.weight(x)\n",
    "        return F.silu(x)*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn \n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        y = self.fn(x, **kwargs)\n",
    "        x = self.drop(x)\n",
    "        return y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, n_embd=n_embd):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, head_dim: int, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = float(head_dim)/(base ** torch.arange(0, head_dim, 2).float())\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.head_dim = head_dim\n",
    "        self.seq_len_cached = None\n",
    "        self.batch_size_cached = None\n",
    "        self.cos_cached: torch.tensor | None = None\n",
    "        self.sin_cached: torch.tensor | None = None\n",
    "\n",
    "    def trig(self, seq_len: int, device=device, dtype=torch.bfloat16) -> torch.Tensor:\n",
    "        if seq_len != self.seq_len_cached: \n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum('i,j -> ij', t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).float().to(device)\n",
    "\n",
    "            self.cos_cached = emb.cos()\n",
    "            self.sin_cached = emb.sin()\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        _, _, seq_len, _ = q.shape\n",
    "        cos, sin = self.trig(seq_len)\n",
    "        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttentionHead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, n_embd, attention_drop = 0.1, ff_drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_drop = nn.Dropout(attention_drop)\n",
    "        self.qkv = nn.Linear(n_embd, n_heads*head_size + 2*head_size)\n",
    "        self.rotary = RotaryEmbedding(head_size)\n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.ff_out = nn.Sequential(\n",
    "            SwiGLU(n_embd=n_embd),\n",
    "            nn.Dropout(ff_drop),\n",
    "            nn.Linear(n_heads * head_size, n_embd, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (B, T, E) ---> (B, num_heads, T, h_size)\n",
    "        B, T, E = x.shape\n",
    "        x = self.LNorm(x)\n",
    "\n",
    "        qkv = self.qkv(x) #(B, T, n_heads*head_size + 2*head_size)\n",
    "\n",
    "        # q has shape (B, n_heads, T, head_size)\n",
    "        q = qkv[:, :, : n_heads*head_size].view((B, T, n_heads, head_size)).transpose(-2, -3)\n",
    "        # k has shape (B, T, head_size)\n",
    "        k = qkv[:, :, n_heads*head_size:n_heads*head_size+head_size].view((B, 1, T, head_size))\n",
    "        # v has shape (B, T, head_size)\n",
    "        v = qkv[:, :, -head_size:].view((B, 1, T, head_size))\n",
    "        \n",
    "        q, k = self.rotary(q, k)\n",
    "\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.1, is_causal=True)\n",
    "        y = einops.rearrange(y, 'b h t d -> b t (h d)')\n",
    "        return self.ff_out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXModel(nn.Module):\n",
    "    def __init__(self, depth, n_heads, head_size, n_embd=n_embd, vocab_size=vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding = Token_Embedding(n_embd = n_embd)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            block = Residual(MultiQueryAttentionHead(n_heads, head_size, n_embd))\n",
    "            self.layers.append(block)\n",
    "        \n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.to_logits = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        #self.to_logits.weight = self.token_embedding.weight\n",
    "        #nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -seq_len:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.token_embedding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        embeds = self.LNorm(x)\n",
    "\n",
    "        logits = self.to_logits(embeds)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, seq_len = seq_len):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.epochs = 5\n",
    "        self.dataset = data(train_data, seq_len)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.model = LaTeXModel(depth, n_heads, head_size)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = lr, weight_decay = wd, betas = betas, eps = eps)\n",
    "        \n",
    "    def train(self):\n",
    "        for iter in range(self.epochs):\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                data, target = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                self.model.train()\n",
    "                _, loss = self.model(data, target)\n",
    "                #if i % 100 == 0:\n",
    "                    #print('{i/}'loss)\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = Trainer()\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaTeXModel(depth, n_heads, head_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = wd, betas = betas, eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss():\n",
    "    eval_iters = 10\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.8071, val loss 0.7963\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 1000\n",
    "for iter in range(10000):\n",
    "    #print training progress\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    #update\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tzen](\\izexedeex; \\m'egr}) ceeak-d})\n",
      "$t gmative) a A$, $esarext-lemmap! th rivedids}. oviccor $.\n",
      "$ orricha(\\id\n",
      "$ lscof $\\b_{By\\sedo on(p.5.R[Se. W)[[Exal exivere-ed. pl theridid\n",
      "Lexisive of$U.\n",
      "Ct, Lesse-d, te$ g= alongrecoherenties, Lemm. The oU(a) {\\it for additivies} if and\n",
      "Cohomology Lemmas \\ref{lemma-direct-composition}.\n",
      "This finish comology on $X$ into $b'$ is an immediate dero,\n",
      "Lemma \\ref{lemma-rescending-charaom-finite-squares}\n",
      "and we can write $\\mathcal{C}$ on $Y$ no\n",
      "a ring map very be opension rank. Name (1) as denotent codimination for $X \\times S \\times_X S$.\n",
      "Asume (1) we obtain the minimalization of points as to structure on the universal\n",
      "stending coherent preserved quasi-cievaluate divisortion} and this.\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{proof}\n",
      "The second arro $X$ is locally with\n",
      "kerne $\\omega_{\\mathfrak p} \\times_{S_S} A_{S_1} \\otimes_{\\mathcal{O}_S}^S_{Y_{S_i}}$ factors\n",
      "and (respec(1)).\n",
      "\\item We that the category ring maps ringed scheme that $\\mathcal{O}_{Y_m\n",
      "\\longrightarrow \\mathcal{F}$. Consider the same\n",
      "$$\n",
      "\\circ \\mathcal{O}_{S_i^*\\Omega_{S_{X/R}} \\otimes_S^\\mathbf{L} R_{K_{\\textit{Coh}(S_X)^{{fpp}} R^{-1}\\mathcal{O}_S}\n",
      "\\oplus \\Omega(\\mathcal{F}) =\n",
      "\\{\n",
      "\\in\n",
      "\\mathcal{O}_S^{-1}\\mathbf{L}\n",
      "K\n",
      "\\to\n",
      "H^{-1}((R_f^*\\bigoplus\\nolimits_{0, y})\n",
      "\\otimes_{R^{-1}} \\otimes_{\\math&2}} D(\\NLambda_{fratorsion})\n",
      "K^{-1}\\mathcal{F}(R)\n",
      "\\\\\n",
      "& \\mit_{(f^{-1}f)}^{-1}Rf_1}K =\n",
      "\\bigoplus\\nolimits_{i =}^{-1}K^{set}h_i\n",
      "$$.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{proof}\n",
      "Part (1) the holds boots for splity on L\\cong R^0$\n",
      "for sheafinite exists\n",
      "wore ner that the functor $\\mathcal{F}_{X/Y}$.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{proof}\n",
      "Let $x \\in \\Ob(\\mathcal{O}_X)$ constructed bet $K$ be a subset $\\mathcal{F}$.\n",
      "The come topologies are if the lattic convention field $x \\to Z$ on $M^*f$\n",
      "and we open way a quotient dimplies (\\ref{equation-universalitive-tensor}),\n",
      "i.e., $\\check_{\\mathcal{F}_j}(\\mathcal{F}_{X/\\mathcal{F}'_T)\\}$\n",
      "of $\\textit{Noetherian}$, $\\mathcal{G}$ and an intersection (1)\n",
      "imal for $f \\in \\Spec(\\mathcal{G})$. See $f = \\ldots$.\n",
      "\\item Setale $X = 1$, $E$, $\\mathcal{F}_\\alpha(\\mathcal{G}'_\\mathcal{F}')$ is\n",
      "an exact simple in $\\mathcal{G}$.\n",
      "By Tomel categories by Lemma \\ref{lemma-stalk-cup-algebraize-modules}\n",
      "we can compute $f_{ij}$ to prod the equivalent\n",
      "veriferlelation with pack $F_i$ is a prespan,\n",
      "\\right). Then $q_i \\in \\mathcal{F}$ by $\\mathcal{L} = 0$,\n",
      "\\in \\{1, \\ldots, t) p$ apothe ring\n",
      "$[\\leq m_r =\n",
      "\\sqm$ with $F_r = \\text{id}(F)$ the form\n",
      "$r\\sum p \\cap f_r(g F|_{T_i})$ (g \\in \\Spec(R))$. Since $\\sigma_1, \\mathcal{F})$\n",
      "are finite type (inperal over $D$ and algebra\n",
      "onto constructed the faniss on the germal sentially fah ot follows, see\n",
      "Clroupoids, Section \\ref{composition-morphisms-schemes}. Fiven for example\n",
      "regular for a $\\mathbf{Z}$ on all Ulying normal depends,\n",
      "in topology index $\\Spec(\\delta) \\to \\\\SH_f(\\mathcal{F}_n)$.\n",
      "Supposks that $2$ is a familiand, and a field ideal. Set $\\{G\\} = \\mathcal{G}_0$\n",
      "and domain map\n",
      "$(\\mathcal{G}) \\in B$ rasum that $v(s)_0$\n",
      "is exactly exact the keyces $\\Spec(k)$ is an open' over $X$.\n",
      "In this a site $P$-dimension $w_\\{0 \\ldots \\lambda} x_{12}$\n",
      "correspondented in the kernel of the second divis to $A$.\n",
      "The functorial the desired and singuishe.\n",
      "Moreover, we definition.\n",
      "We can reverd by theorem-hasisly CRS2 is an\n",
      "\\begimentale over $T$. If $J$.\n",
      "Because $\\text{Supp}_d : S \\to U$ with $T' = [T'_d]$ are the factorization homization.\n",
      "As image (progral inking $T_i) = \\subset M$.\n",
      "Then we all the dysumption we let\n",
      "$\\alpha$ is an $M \\in D^{+}(T)$ be isomorphism.\n",
      "\n",
      "For any $\\varphi$ we can let $V_d$ be the denoted aived complet construction\n",
      "on $T^i$ maps and the sense a subset $\\mathcal{C}$ at the module\n",
      "$$\n",
      "\\text{frm}(P) = \\text{\\text{Coum}_{apply}(\\mathcal{U}, \\mathcal{C} I \\text{ and\n",
      "we also\n",
      "have that $w \\to (\\Omega_{V \\times_(X_1 \\times_X \\times \\mathcal{C}, (X_f)) = X \\to S \\times_X X$\n",
      "is the length base change asismat to addition sheaf $x$ are isomorphist\n",
      "of the proposition o\\text{Sec}_{S/S}$ is actually). Hi& we conclude\n",
      "$U \\to \\Omega_{X/X}^\\bullet \\sight)$ is zero.\n",
      "\n",
      "\\medskip\\noindent\n",
      "Let $g \\glo. Let $U \\times_S S$ be a commutative diagram\n",
      "of local rings, when has the map\n",
      "$$\n",
      "g = \\lim_{S \\otimes_S_S S \\S!} |U|_{\\mathcal{F}}\n",
      "(U \\times_S X)\n",
      "= \\Ghow\\quadsut\n",
      "\\Mor_{(U/X}|_S) \\to\n",
      "s\\Spec(S, \\Sh(S)/g)_{U \\to (\\Sch/S}),\n",
      "\\\\\n",
      "$$\n",
      "is the any nonstract this sheaf of paper of rational mected.\n",
      "\n",
      "\\medskip\\noindent\n",
      ". By Lemma \\ref{lemma-is-abbsolutions-to-first-fying-locally-vangent-tein-asstrized}\n",
      "and has Theonemproof (premay) it given by from\n",
      "the category $\\{(U_0, \\Spec(S_0), \\mathcal{O}_0(\\mathcal{O}_{R, \\gambdalpha)}) +\n",
      "d (\\S_1)\\}\n",
      "$$\n",
      "by the {Lelat omitted cobtained\n",
      "by show\n",
      "in the functor $\\omega_{\\overline{k}/X_0}$ and chow imilarizers of $X$\n",
      "in $\\mathfrak p \\in Y$, largelogy embed modules on $\\mathcal{F}$.\n",
      "Thus the map\n",
      "$$\n",
      "if $x_0 \\in x_0 \\ldots \\to x_0 =\n",
      "\\Spec(\\mathcal{G} = \\mathcal{O}V_0 \\ldots\n",
      "$$\n",
      "generate the strictude where $H_2, \\delta$ is\n",
      "a ration flat isomain open neeld on $k \\to X\\}_\\beta$ by\n",
      "$k$ and other over $y_{i \\in \\mathcal{G}}^\\bullet \\varphi'_0 \\to \\mathcal{G}^\\wedge$\n",
      "where $\\sigma(\\mathcal{G}^\\bullet) \\zetale'_\\bullet$ is over $\\corluin \\bigcup_i$\n",
      "as proof we see that $\\xi$, then $\\xira$ is {\\it stalk formall there the\n",
      "same Cohomology}\n",
      "$n_i$ of the {\\\"im$-adic define tors $\\xi_i : \\xi_0 \\ldots, c \\mathcal{F}_c$ mint\\delta_{Y_i}(a_i)$\n",
      "define the {\\it the duality minimal posduls faithfully $\\xi_{i, \\xi \\in J}$.\n",
      "\\end{enumerate}\n",
      "\\end{definition}\n",
      "\n",
      "\\begin{definition}\n",
      "\\label{definition-Valtering}\n",
      "Let $A$ be a ring. Let $X$ be localization of commutative ringed sum} of $S$\n",
      "and we cohow find the Groupoids of $B$ and $\\xi_{i, 0}$.\n",
      "Then $\\delta(a, b) \\'$\n",
      "is a ring. \\end{definition-compbreprgye-local} if and $a$\n",
      "is bile, then $R \\to R\\Spec(A) \\to R$ is algebra homomorphism at\n",
      "the eferench $a_*_*x(a_\\betaR)$ iso for all to\n",
      "$v(x)\\{f$ have anmodular\n",
      "$f \\in \\Cokeng \\QC_{\\xi}}(A_\\etale})$.\n",
      "Let $f : L \\to M$ the $\\xi$. The procebal ing ideal of the set of $F^\\bullet$.\n",
      "Vove we use the slative dual\n",
      "if $f^--2}$ is flat we ideal. The branks $Y_s/fA$\n",
      "has the functor is that $f^{-1}\\mathcal{F}$. Moreover,\n",
      "indicable for every cartum\n",
      "$\\pi^{M I}$ of sheaves of $\\mathcal{G}$ over any because\n",
      "$(f^*\\bulle\n",
      "\\Hom(S_y, R) \\to \\HomR\\Gamma(D(f|_f^*\\mathcal{F})$) so $x : R'_{(f_{x + 1}^*\\mathcal{F}(f \\oplus\n",
      "D \\to \\text{Mod}_{\\mathcal{O}_{F_0\\mathcal{F}_{\\mathcal{F}_\\mathcal{G}_y}}(f_1^{\\mathcal{F})}$.\n",
      "Similarly, then we using $f$ by the map\n",
      "\\begin{enumerate}\n",
      "\\item with the sill invertible composible support of the lemma.\n",
      "\\item Let $E_0^\\infty$ be a stroduct surjective and $\\Omega_2/\\mathcal{F}$\n",
      "and the fibre open $\\mathcal{F}$\n",
      "over $k$ is complete of $\\mathcal{O}_X$-modules.\n",
      "\\end{theorem}\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{lemma-section-e1-2-flattem}\n",
      "Let $(X, Y, \\mathcal{O}_X)$ be as a quasi-coherent spaces\n",
      "of (Section\n",
      "\\ref{section-bmanding-transfunctor}) assume on $(\\mathcal{O}_X)$ and\n",
      "$\\mathcal{V}_Y$ morphisms of zero (\\ref{equation-relative-bas-lase-change-cohomology}).\n",
      "Finish that $\\Sh(S)_f$ is geometricated by as {\\item $g, \\Lambda}$ open\n",
      "$g(f)_f$} bivala Topose sets $\\Spec(k[f, f])$, see\n",
      "Lemma \\ref{lemma-unill-ten-descending-module-module} apply\n",
      "Mittagains that and object.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{lemma-flat-resore-proposition}\n",
      "Let $R$ be a ring. Let $R$ be a scheme. Let $E \\subset R$\n",
      "be a ring. Then $K^\\bullet = \\mathbf{Z}/R$ of $\\Spec(n)$\n",
      "which globally of flat and $F : \\Spec(B)$ is a field in the category\n",
      "on groupoids for $Z, Z' = X \\in \\cap \\Spm(n \\times s)$. By induction extensions\n",
      "of the homology gree $\\text{Tot}(Pose wee\n",
      "conclude that (1). Hence $f$ is a ring $\\overline{X}$.\n",
      "Thus (1) $i : \\ldots \\times 1 \\lepsiltots f(1)$ cit is\n",
      "cle a function by an isomorphisme all are follows universal.\n",
      "Stect.\n",
      "\n",
      "\\end{proof}\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{lemma-commutes-duality-zn}\n",
      "This morphisms in the suppoot of inverse this that means as by\n",
      "Lemma \\ref{lemma-najection-inf-homomer} also\n",
      "(Let, Lemma \\ref{limits-modulo-local}).\n",
      "Then $\\lim_i \n",
      "\\Kor_= K_i \\to D(K_i\\text{Mod}_i \\to \\Hom^{\\tau}_j}(K_{\\xi, K_{\\textii} \\otimes 0})$\n",
      "be equationle of $K_i$. Hence we can be we work on $F_{i_i}(f_i)$ tadent\n",
      "of the finite squarition (ite:\n",
      "Definition) : I. We necall work' as in (2) and that\n",
      "$f^{i + 1}(K : D(f_j, \\xi_{\\textit{Mod}_X) \\to F_S$ has defined.\n",
      "Using the $R \\to I$ and hence $AF \\to Y$ in $I$ is $P_i$ vious the econe descent subver\n",
      "from $M_{\\text{Mod}_S}(M)$, use Descrition\n",
      "\\ref{definition-Hom-K-goren\n",
      "by-defines $A \\subset \\text{ds}_{d \\in \\mathbf{dotu}_S, M}$.\n",
      "Then then $M$ is binant dasy we\n",
      "can closed extension of sheaf $Z \\times T$ and back $M_{d_1} \\to M_{rk}$\n",
      "a finish descent subsets of $\\sciar N^{d - + 1}$.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{proof}\n",
      "We conclude that the looking Cohomology construction of $F_{repare rings tor-flan}$\n",
      "in $D_{a(T_1})$ if $Qense $S$, $g \\circ C$ maps $\\xi$, gyiving the result.\n",
      "\\end{example}\n",
      "\n",
      "\\label{example-normation-canons-N-Toubits}\n",
      "Let $R$ be an'.\n",
      "Let $R$ be a Noetherme of then anormal aldomorphism of de{``big algebra modult, and equivalence.\n",
      "Final, the base change $M$ in $R$-algebra exthend ampletionaded by\n",
      "adequalized notion.\n",
      "As $0 \\to \\textit{Mod}(M \\to R)$ we. Note the result for rings an ourting $R/M$\n",
      "is an (finitely point and dual we more cally $\\mathbf{M}$ us relative invertible points\n",
      "of $\\mathbf{Z}[x]$.\n",
      "\\end{lemma}\n",
      "\n",
      "\\noindent\n",
      "We show will use the resulct of dimensionis has thinks exists a xt addition by\n",
      "exact, theors and we fet nonemptypen-geometric as an into prove shrimplicial\n",
      "Kostan.) Hill divided\n",
      "to rided in $A$ is a morphism $\\tau$-module we have\n",
      "closed projisment poly primes of $A$.\n",
      "\n",
      "\\medskip\\noindent\n",
      "Let $\\left'$ be tw be an \\'etale numbigral locally neighbourhood of $\\delta(g_X)$.\n",
      "\n",
      "\\begin{enumerate}\n",
      "\\item If $R$ is Noether-Scheme, $\\text{Noetherian}_X$ and $[(geteriz]) the precent $\\tex{Mod}_{\\text{d}m}_1/S}$\n",
      "is a finite graded object. Observel/n$ of we exactly monctor $R$ and\n",
      "restriction affine is perfect and $n' \\in M$ by universally holds, the functor\n",
      "on $M$ as an by derod $\\mathcal{O}_{\\kappa}$ or $\\mathcal{F}_{P_*}$\n",
      "$F^{(K)}(f) \\to \\mathcal{F}_2)^\\shes}$ produc\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 128), dtype=torch.long).to(device)\n",
    "print(detokenize(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6928896"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, r'C:\\Users\\micah\\Desktop\\pytorch_models\\falcon256')\n",
    "#model = torch.load(r'C:\\Users\\micah\\Desktop\\pytorch_models\\falcon256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
