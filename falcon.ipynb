{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Darrell/Desktop/tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for i, s in enumerate(vocab)}\n",
    "tokenize = lambda s: [stoi[c] for c in s]\n",
    "detokenize = lambda c: ''.join([itos[x] for x in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, head_dim: int, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = float(head_dim)/(base ** torch.arange(0, head_dim, 2).float())\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.head_dim = head_dim\n",
    "        self.seq_len_cached = None\n",
    "        self.batch_size_cached = None\n",
    "        self.cos_cached: torch.tensor | None = None\n",
    "        self.sin_cached: torch.tensor | None = None\n",
    "\n",
    "    def trig(self, seq_len: int, device=device, dtype=torch.bfloat16) -> torch.Tensor:\n",
    "        if seq_len != self.seq_len_cached: \n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum('i,j -> ij', t, self.inv_freq)\n",
    "            emb = torch.cat((freq, freq), dim=-1).float().to(device)\n",
    "\n",
    "            self.cos_cached = emb.cos()[None, :, :]\n",
    "            self.sin_cached = emb.sin()[None, :, :]\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #need to decide if this should take (q, k) or just x\n",
    "        pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, n_embd, attention_drop = 0.1, residual_drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention_drop = attention_drop\n",
    "        self.residual_drop = residual_drop\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (B, T, E) ---> (B, num_heads, T, h_size)\n",
    "        B, T, E = x.shape\n",
    "        q = self.query(x).view(B, T, n_head, h_size).transpose(1, 2)\n",
    "        k = self.key(x).view(B, T, n_head, h_size).transpose(1, 2).transpose(-1, -2)\n",
    "        W = q @ k * E ** -0.5\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things to implement:\n",
    "#   1. Flash Attention\n",
    "#   2. Multi-Query Attention\n",
    "#   3. Rotary Positional Embedding\n",
    "#   4. GLU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8\n",
    "seq_len = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_freq = 1.0 / (10000 ** (torch.arange(0, 8, 2).float() / 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.1000, 0.0100, 0.0010])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2857, 0.4643, 0.6429, 0.8214])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(seq_len).type_as(inv_freq)\n",
    "freqs = torch.einsum('i , j -> i j', t, inv_freq)\n",
    "freqs = torch.cat((freqs, freqs), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03, 1.0000e+00, 1.0000e-01,\n",
       "         1.0000e-02, 1.0000e-03],\n",
       "        [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03, 2.0000e+00, 2.0000e-01,\n",
       "         2.0000e-02, 2.0000e-03],\n",
       "        [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03, 3.0000e+00, 3.0000e-01,\n",
       "         3.0000e-02, 3.0000e-03]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum('i,j -> ij', t, self.inv_freq)\n",
    "            emb = torch.cat((freq, freq), dim=-1).float().to(device)\n",
    "\n",
    "            self.cos_cached = emb.cos()[None, :, :]\n",
    "            self.sin_cached = emb.sin()[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_freq = 8.0/(10 ** torch.arange(0, 8, 2).float())\n",
    "t = torch.arange(5)\n",
    "freq = torch.einsum('i, j -> ij', t, inv_freq)\n",
    "emb = torch.cat((freq, freq), dim=-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.ones((2, 3, 4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4*3*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
