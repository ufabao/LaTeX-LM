{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Darrell/Desktop/tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 3\n",
    "n_embd = 5\n",
    "head_size = 8\n",
    "n_heads = 11\n",
    "depth = 6\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 1e-2\n",
    "betas = (0.9, 0.99)\n",
    "eps = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for i, s in enumerate(vocab)}\n",
    "tokenize = lambda s: [stoi[c] for c in s]\n",
    "detokenize = lambda c: ''.join([itos[x] for x in c])\n",
    "\n",
    "data = torch.tensor(tokenize(text))\n",
    "n = int(len(data)*.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+1 + seq_len] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn \n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        y = self.fn(x, **kwargs)\n",
    "        x = self.drop(x)\n",
    "        return y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, n_embd=n_embd):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, head_dim: int, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = float(head_dim)/(base ** torch.arange(0, head_dim, 2).float())\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.head_dim = head_dim\n",
    "        self.seq_len_cached = None\n",
    "        self.batch_size_cached = None\n",
    "        self.cos_cached: torch.tensor | None = None\n",
    "        self.sin_cached: torch.tensor | None = None\n",
    "\n",
    "    def trig(self, seq_len: int, device=device, dtype=torch.bfloat16) -> torch.Tensor:\n",
    "        if seq_len != self.seq_len_cached: \n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum('i,j -> ij', t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).float().to(device)\n",
    "\n",
    "            self.cos_cached = emb.cos()\n",
    "            self.sin_cached = emb.sin()\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        _, _, seq_len, _ = q.shape\n",
    "        cos, sin = self.trig(seq_len)\n",
    "        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttentionHead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, n_embd, attention_drop = 0.1, ff_drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_drop = nn.Dropout(attention_drop)\n",
    "        self.qkv = nn.Linear(n_embd, n_heads*head_size + 2*head_size)\n",
    "        self.rotary = RotaryEmbedding(head_size)\n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.ff_out = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(ff_drop),\n",
    "            nn.Linear(n_heads * head_size, n_embd, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (B, T, E) ---> (B, num_heads, T, h_size)\n",
    "        B, T, E = x.shape\n",
    "        x = self.LNorm(x)\n",
    "\n",
    "        #TODO: see if this qkv thing is actually faster than doing seperate q, k, v \n",
    "        qkv = self.qkv(x) #(B, T, n_heads*head_size + 2*head_size)\n",
    "\n",
    "        # q has shape (B, n_heads, T, head_size)\n",
    "        q = qkv[:, :, : n_heads*head_size].view((B, T, n_heads, head_size)).transpose(-2, -3)\n",
    "        # k has shape (B, T, head_size)\n",
    "        k = qkv[:, :, n_heads*head_size:n_heads*head_size+head_size].view((B, 1, T, head_size))\n",
    "        # v has shape (B, T, head_size)\n",
    "        v = qkv[:, :, -head_size:].view((B, 1, T, head_size))\n",
    "        \n",
    "        q, k = self.rotary(q, k)\n",
    "\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.1, is_causal=True)\n",
    "        y = einops.rearrange(y, 'b h t d -> b t (h d)')\n",
    "        return self.ff_out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXModel(nn.Module):\n",
    "    def __init__(self, depth, n_heads, head_size, n_embd=n_embd, vocab_size=vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding = Token_Embedding(n_embd = n_embd)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            block = Residual(MultiQueryAttentionHead(n_heads, head_size, n_embd))\n",
    "            self.layers.append(block)\n",
    "        \n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.to_logits = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        #self.to_logits.weight = self.token_embedding.weight\n",
    "        #nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.token_embedding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        embeds = self.LNorm(x)\n",
    "\n",
    "        logits = self.to_logits(embeds)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaTeXModel(depth, n_heads, head_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = wd, betas = betas, eps = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3521, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3362, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for iter in range(2):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        #losses = estimate_loss()\n",
    "        #print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    print(loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "#context = torch.zeros((64, 256), dtype=torch.long)\n",
    "#print(detokenize(model.generate(context, max_new_tokens=50)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((64, 3, 88))\n",
    "y, z = x.chunk(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 44])"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 44])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 44])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.silu(z) * y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
