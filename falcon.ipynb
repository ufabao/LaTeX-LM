{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"\\Users\\micah\\Desktop\\tiny-shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 256\n",
    "n_embd = 512\n",
    "head_size = 64\n",
    "n_heads = 8\n",
    "depth = 6\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 1e-2\n",
    "betas = (0.9, 0.99)\n",
    "eps = 1e-8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for i, s in enumerate(vocab)}\n",
    "tokenize = lambda s: torch.tensor([stoi[c] for c in s])\n",
    "detokenize = lambda c: ''.join([itos[x] for x in c])\n",
    "\n",
    "data = tokenize(text)\n",
    "n = int(len(data)*.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "    def __init__(self, text, seq_len=seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.data = text\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx: idx + seq_len], self.data[idx+1: idx + seq_len + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Linear(n_embd, n_embd)\n",
    "    def forward(self, x):\n",
    "        y = self.weight(x)\n",
    "        return F.silu(x)*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn \n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        y = self.fn(x, **kwargs)\n",
    "        x = self.drop(x)\n",
    "        return y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, n_embd=n_embd):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, head_dim: int, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = float(head_dim)/(base ** torch.arange(0, head_dim, 2).float())\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.head_dim = head_dim\n",
    "        self.seq_len_cached = None\n",
    "        self.batch_size_cached = None\n",
    "        self.cos_cached: torch.tensor | None = None\n",
    "        self.sin_cached: torch.tensor | None = None\n",
    "\n",
    "    def trig(self, seq_len: int, device=device, dtype=torch.bfloat16) -> torch.Tensor:\n",
    "        if seq_len != self.seq_len_cached: \n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum('i,j -> ij', t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).float().to(device)\n",
    "\n",
    "            self.cos_cached = emb.cos()\n",
    "            self.sin_cached = emb.sin()\n",
    "\n",
    "        return self.cos_cached, self.sin_cached\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        _, _, seq_len, _ = q.shape\n",
    "        cos, sin = self.trig(seq_len)\n",
    "        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttentionHead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, n_embd, attention_drop = 0.1, ff_drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_drop = nn.Dropout(attention_drop)\n",
    "        self.qkv = nn.Linear(n_embd, n_heads*head_size + 2*head_size)\n",
    "        self.rotary = RotaryEmbedding(head_size)\n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.ff_out = nn.Sequential(\n",
    "            SwiGLU(n_embd=n_embd),\n",
    "            nn.Dropout(ff_drop),\n",
    "            nn.Linear(n_heads * head_size, n_embd, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = (B, T, E) ---> (B, num_heads, T, h_size)\n",
    "        B, T, E = x.shape\n",
    "        x = self.LNorm(x)\n",
    "\n",
    "        qkv = self.qkv(x) #(B, T, n_heads*head_size + 2*head_size)\n",
    "\n",
    "        # q has shape (B, n_heads, T, head_size)\n",
    "        q = qkv[:, :, : n_heads*head_size].view((B, T, n_heads, head_size)).transpose(-2, -3)\n",
    "        # k has shape (B, T, head_size)\n",
    "        k = qkv[:, :, n_heads*head_size:n_heads*head_size+head_size].view((B, 1, T, head_size))\n",
    "        # v has shape (B, T, head_size)\n",
    "        v = qkv[:, :, -head_size:].view((B, 1, T, head_size))\n",
    "        \n",
    "        q, k = self.rotary(q, k)\n",
    "\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.1, is_causal=True)\n",
    "        y = einops.rearrange(y, 'b h t d -> b t (h d)')\n",
    "        return self.ff_out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTeXModel(nn.Module):\n",
    "    def __init__(self, depth, n_heads, head_size, n_embd=n_embd, vocab_size=vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.token_embedding = Token_Embedding(n_embd = n_embd)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            block = Residual(MultiQueryAttentionHead(n_heads, head_size, n_embd))\n",
    "            self.layers.append(block)\n",
    "        \n",
    "        self.LNorm = LayerNorm(n_embd)\n",
    "        self.to_logits = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        #self.to_logits.weight = self.token_embedding.weight\n",
    "        #nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -seq_len:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.token_embedding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        embeds = self.LNorm(x)\n",
    "\n",
    "        logits = self.to_logits(embeds)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, seq_len = seq_len):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.epochs = 5\n",
    "        self.dataset = data(train_data, seq_len)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.model = LaTeXModel(depth, n_heads, head_size)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = lr, weight_decay = wd, betas = betas, eps = eps)\n",
    "        \n",
    "    def train(self):\n",
    "        for iter in range(self.epochs):\n",
    "            for i, batch in enumerate(self.dataloader):\n",
    "                data, target = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                self.model.train()\n",
    "                _, loss = self.model(data, target)\n",
    "                #if i % 100 == 0:\n",
    "                    #print('{i/}'loss)\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer()\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[41], line 21\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m#if i % 100 == 0:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[39m#print('{i/}'loss)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGCIOvStoMC\n",
      "WncaOCLOPOROtMWerNGoCulaftswaWioOfrosowhLonUKIOHiuavo,OviGLABUKaromilprdo'Gres hed HERARNolonovascarond:\n",
      "Folounincond?\n",
      "Tond SCUCetel myopto'st uourersh!\n",
      "DUSTUTIO:\n",
      "Ser,\n",
      "A LoGod, thary 'Wham in tlaut onot anee eartle therery what buth ista's ur heart,\n",
      "The father's to you lovercy? dive disposs pon\n",
      "Look ison. O's hid in thou chatly canised\n",
      "A not, in so for gron; he nothan come\n",
      "Your ther side mame minegualdner hou woul at\n",
      "There blown by thee falk spoke of\n",
      "Affect the joy ly counter trive o\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 128), dtype=torch.long).to(device)\n",
    "print(detokenize(trainer.model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
